{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install requests beautifulsoup4 google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHf-8KuX2Y5v",
        "outputId": "8cc37d9d-1a20-4a34-f710-1cb95bb27426"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.170.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhMnRhms2dWG",
        "outputId": "8dcfa97e-20a4-45cd-a45f-b755db912053"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n"
      ],
      "metadata": {
        "id": "6_z2-H5L2m5k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build"
      ],
      "metadata": {
        "id": "B4XH7aVLAGk0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziTtXkMnv3cg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import time\n",
        "from io import BytesIO\n",
        "from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload\n",
        "\n",
        "# Update this path to where your service account JSON key is located\n",
        "SERVICE_ACCOUNT_FILE = '/content/sunny-strategy-446220-m9-41cbb114efa5.json'\n",
        "\n",
        "# Your Google Drive file ID for the text file that contains the start URL\n",
        "\n",
        "FILE_ID = '1q30D3P03lgPciN2D41xjn1jQDbk-ZwBU'\n",
        "\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
        "\n",
        "visited_urls = set()\n",
        "\n",
        "def is_valid_url(url):\n",
        "    try:\n",
        "        result = urlparse(url)\n",
        "        return all([result.scheme, result.netloc])\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def get_domain(url):\n",
        "    return urlparse(url).netloc\n",
        "\n",
        "def scan_website_recursively(start_url, base_domain, max_depth, current_depth=0):\n",
        "    internal_links = []\n",
        "    if current_depth > max_depth:\n",
        "        return internal_links\n",
        "\n",
        "    normalized_url = urljoin(start_url, urlparse(start_url).path)\n",
        "    if normalized_url in visited_urls:\n",
        "        return internal_links\n",
        "\n",
        "    visited_urls.add(normalized_url)\n",
        "    print(f\"Crawling: {normalized_url} (Depth: {current_depth})\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(normalized_url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to fetch {normalized_url}: {e}\")\n",
        "        return internal_links\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    for tag in soup.find_all('a', href=True):\n",
        "        href = tag['href']\n",
        "        full_url = urljoin(normalized_url, href)\n",
        "        clean_url = urljoin(full_url, urlparse(full_url).path)\n",
        "\n",
        "        if not is_valid_url(clean_url):\n",
        "            continue\n",
        "\n",
        "        if get_domain(clean_url) == base_domain and clean_url not in visited_urls:\n",
        "            internal_links.append(clean_url)\n",
        "            internal_links += scan_website_recursively(clean_url, base_domain, max_depth, current_depth + 1)\n",
        "\n",
        "    time.sleep(0.5)  # polite delay to avoid hammering the server\n",
        "    return internal_links\n",
        "\n",
        "def get_url_from_drive():\n",
        "    creds = service_account.Credentials.from_service_account_file(\n",
        "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
        "    service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "    request = service.files().get_media(fileId=FILE_ID)\n",
        "    fh = BytesIO()\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "    done = False\n",
        "    while not done:\n",
        "        status, done = downloader.next_chunk()\n",
        "\n",
        "    url_text = fh.getvalue().decode(\"utf-8\").strip()\n",
        "    print(f\"Start URL read from Drive file: {url_text}\")\n",
        "    return url_text\n",
        "\n",
        "def upload_to_drive(content):\n",
        "    creds = service_account.Credentials.from_service_account_file(\n",
        "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
        "    service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "    file_metadata = {'name': 'crawled_links.txt', 'mimeType': 'text/plain'}\n",
        "    fh = BytesIO(content.encode('utf-8'))\n",
        "    media = MediaIoBaseUpload(fh, mimetype='text/plain')\n",
        "    uploaded_file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
        "    print(f\"Uploaded crawled links file to Drive with File ID: {uploaded_file['id']}\")\n",
        "    return uploaded_file['id']\n",
        "\n",
        "def main():\n",
        "    start_url = get_url_from_drive()\n",
        "    if not is_valid_url(start_url):\n",
        "        print(\"Invalid URL in Drive file.\")\n",
        "        return\n",
        "\n",
        "    base_domain = get_domain(start_url)\n",
        "    print(f\"Crawling domain: {base_domain}\")\n",
        "\n",
        "    results = scan_website_recursively(start_url, base_domain, max_depth=2)\n",
        "    print(f\"Found {len(results)} internal links.\")\n",
        "\n",
        "    content = \"\\n\".join(results)\n",
        "    upload_to_drive(content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KLjcjXYI99Fh",
        "outputId": "d9913116-82c7-4ee0-f760-b2d9e12fea00"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start URL read from Drive file: https://www.geeksforgeeks.org/\n",
            "Crawling domain: www.geeksforgeeks.org\n",
            "Crawling: https://www.geeksforgeeks.org/ (Depth: 0)\n",
            "Crawling: https://www.geeksforgeeks.org/learn-data-structures-and-algorithms-dsa-tutorial/ (Depth: 1)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/dsa-to-development-coding-guide/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/category/ibm-certification/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/mastering-django-framework-beginner-to-advance/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/search (Depth: 2)\n",
            "Failed to fetch https://www.geeksforgeeks.org/courses/search: 500 Server Error: Internal Server Error for url: https://www.geeksforgeeks.org/courses/search\n",
            "Crawling: https://www.geeksforgeeks.org/courses/interviewe-101-data-structures-algorithm-system-design/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/full-stack-applied-data-science-program/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/Java-backend-live (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/devops-live (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/Data-Structures-With-Python (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/placement-prep-live (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/data-science-live (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/dsa-self-paced (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/competitive-programming-cp (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/full-stack-node (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/ai-ml-and-data-science-tutorial-learn-ai-ml-and-data-science/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/interview-corner/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/programming-language-tutorials/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/web-technology/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/articles-on-computer-science-subjects-gq/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/devops-and-linux-tutorial/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/geeksforgeeks-school/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/courses/gfg-160-series (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/problem-of-the-day (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/geeksforgeeks-practice-best-online-coding-platform/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/sde-sheet-a-complete-guide-for-sde-preparation/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/data-structures/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/fundamentals-of-algorithms/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/complete-guide-to-arrays-data-structure/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/complete-guide-to-string-data-structure/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/linked-list-data-structure/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/stack-data-structure/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/queue-data-structure/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/introduction-to-tree-data-structure-and-algorithm-tutorials/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/graph-data-structure-and-algorithms/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/searching-algorithms/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/sorting-algorithms/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/introduction-to-recursion-2/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/dynamic-programming/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/binary-tree-data-structure/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/binary-search-tree-data-structure/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/heap-data-structure/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/hashing-data-structure/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/divide-and-conquer/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/mathematical-algorithms/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/geometric-algorithms/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/bitwise-algorithms/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/greedy-algorithms/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/backtracking-algorithms/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/branch-and-bound-algorithm/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/matrix/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/pattern-searching/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/randomized-algorithms/ (Depth: 2)\n",
            "Crawling: https://www.geeksforgeeks.org/array-data-structure-guide/ (Depth: 2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-07012b86f2b6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-07012b86f2b6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Crawling domain: {base_domain}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_website_recursively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_domain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found {len(results)} internal links.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-07012b86f2b6>\u001b[0m in \u001b[0;36mscan_website_recursively\u001b[0;34m(start_url, base_domain, max_depth, current_depth)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_url\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbase_domain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mclean_url\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvisited_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0minternal_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0minternal_links\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mscan_website_recursively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_domain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# polite delay to avoid hammering the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-07012b86f2b6>\u001b[0m in \u001b[0;36mscan_website_recursively\u001b[0;34m(start_url, base_domain, max_depth, current_depth)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_url\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbase_domain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mclean_url\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvisited_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0minternal_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0minternal_links\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mscan_website_recursively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_domain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# polite delay to avoid hammering the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-07012b86f2b6>\u001b[0m in \u001b[0;36mscan_website_recursively\u001b[0;34m(start_url, base_domain, max_depth, current_depth)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0minternal_links\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mscan_website_recursively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_domain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# polite delay to avoid hammering the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minternal_links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}